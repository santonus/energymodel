# -*- coding: utf-8 -*-
"""logisticRegression

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ld1NqXPar74FCPrL0t-EKBayCFb8XHM
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

from google.colab import drive
drive.mount('/content/gdrive')

df=pd.read_csv('/content/Power Model Inputs - Kepler_data_original.csv')
print(df.columns)

df1=df.drop(['Sr No.','Benchmark','Kernel Name','C/M','Dwarf Type'],axis=1)

df1=df1[df1['avg_shar_lat']!='#DIV/0!']
df1=df1[df1['avg_shar_lat']!=' ']
df1=df1[df1['avg_glob_lat']!='#DIV/0!']
df1=df1[df1['avg_misc_lat']!='#DIV/0!']
df1=df1[df1['avg_comp_lat']!='#DIV/0!']

df1=df1[df1['avg_shar_lat']!='#REF!']

df1['avg_shar_lat']=df1['avg_shar_lat'].astype('float64')
df1['avg_glob_lat']=df1['avg_glob_lat'].astype('float64')
df1['avg_comp_lat']=df1['avg_comp_lat'].astype('float64')
df1['avg_misc_lat']=df1['avg_misc_lat'].astype('float64')
df1

df1=df1.drop(['wtotal','Global_Throughput','sh_throughput','total_inst','comp_lat_sim','glob_lat_sim','misc_lat_sim','shar_lat_sim','glob_inst_sim','shar_inst_sim','misc_inst_sim','comp_inst_sim','glb_penalty','waves','grid_size','Total Threads','sm_active','shar_inst_kernel','sh_penalty','avg_misc_lat','n_warps'],axis=1)
df1=df1.dropna(axis=0,how='any')
df1

y_train=df1['Power']
y_train=np.array(y_train).astype('float32')
y_train.shape
y_train

x_train=df1.iloc[:,0:15]
x_train=np.array(x_train).astype('float32')
#x_train=np.array(x_train).astype('Int32')
x_train

df1.columns

#data scaling
from sklearn import preprocessing
min_max_scaler = preprocessing.StandardScaler()
X_scale = min_max_scaler.fit_transform(x_train)
print(X_scale.shape)


#splitting the data for training and validation test
from sklearn.utils import shuffle
X_scale,y_train = shuffle(X_scale, y_train, random_state=0)
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_scale, y_train, test_size=0.2)
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

logreg = LogisticRegression(solver='liblinear', random_state=0,max_iter=10)
logreg.fit(X_train,Y_train)
y_pred=logreg.predict(X_test)
from sklearn import metrics
cnf_matrix = metrics.confusion_matrix(Y_test, y_pred)
cnf_matrix

print("Accuracy:",metrics.accuracy_score(Y_test, y_pred))
print("Precision Score : ",metrics.precision_score(Y_test, y_pred,  average='weighted'))
print("Recall Score : ",metrics.recall_score(Y_test, y_pred,  average='weighted'))

from sklearn import neighbors
from sklearn.metrics import mean_squared_error
from math import sqrt
import matplotlib.pyplot as plt
rsquared=[]
rmse_val = [] #to store rmse values for different k
for K in range(20):
    K = K+1
    model = neighbors.KNeighborsRegressor(n_neighbors = K)

    model.fit(X_train, Y_train)  #fit the model
    pred=model.predict(X_test) #make prediction on test set
    error = sqrt(mean_squared_error(Y_test,pred)) #calculate rmse
    rmse_val.append(error) #store rmse values
    print('RMSE value for k= ' , K , 'is:', error)
    rsquared.append(model.score(X_test, Y_test))
    print('R-squared test score: {:.3f}'.format(model.score(X_test, Y_test)))
    curve = pd.DataFrame(rmse_val) #elbow curve
    curve.plot()

from sklearn.model_selection import GridSearchCV
params = {'n_neighbors':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],
           'weights': ['uniform', 'distance']}
from sklearn import neighbors
knn = neighbors.KNeighborsRegressor()

model = GridSearchCV(knn, params, cv=5)
model.fit(X_scale,y_train)
model.best_params_

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_scale, y_train, cv=5)
print(scores)

print(scores.mean())

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_scale, y_train, cv=5,scoring='neg_root_mean_squared_error')
print(scores)

print(scores.mean())

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X_scale, y_train, cv=5,scoring='neg_mean_absolute_error')
print(scores)

print(scores.mean())