# -*- coding: utf-8 -*-
"""GradientBoosting.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XF8Mk5ww0bcuYN8hMegx-EjMJuA13Hqi
"""

import numpy as np
import pandas as pd

from google.colab import drive
drive.mount('/content/gdrive')
df=pd.read_csv('/content/gdrive/MyDrive/Colab Notebooks/KeplerHDBSCAN.csv')
print(df.columns)

df1=df.drop(['Sr No.','Benchmark','Kernel Name'],axis=1)

df1=df1[df1['avg_shar_lat']!='#DIV/0!']
df1=df1[df1['avg_shar_lat']!=' ']
df1=df1[df1['avg_glob_lat']!='#DIV/0!']
df1=df1[df1['avg_misc_lat']!='#DIV/0!']
df1=df1[df1['avg_comp_lat']!='#DIV/0!']

df1=df1[df1['avg_shar_lat']!='#REF!']

df1['avg_shar_lat']=df1['avg_shar_lat'].astype('float64')
df1['avg_glob_lat']=df1['avg_glob_lat'].astype('float64')
df1['avg_comp_lat']=df1['avg_comp_lat'].astype('float64')
df1['avg_misc_lat']=df1['avg_misc_lat'].astype('float64')
df1

df1=df1.drop(['wtotal','Global_Throughput','sh_throughput','total_inst','comp_lat_sim','glob_lat_sim','misc_lat_sim','shar_lat_sim','glob_inst_sim','shar_inst_sim','misc_inst_sim','comp_inst_sim','glb_penalty','waves','grid_size','Total Threads','sm_active','shar_inst_kernel','sh_penalty','avg_misc_lat','n_warps'],axis=1)
df1=df1.dropna(axis=0,how='any')
df1
df2=df1.drop(['Power'],axis=1)
y_train=df1['Power']
y_train=np.array(y_train).astype('float32')
y_train.shape
x_train=df1.iloc[:,0:15]
x_train=np.array(x_train).astype('float32')
x_train

from sklearn import preprocessing
min_max_scaler = preprocessing.MinMaxScaler()
X_scale = min_max_scaler.fit_transform(x_train)
print(X_scale.shape)

#splitting the data for training and validation test
from sklearn.utils import shuffle
X_scale,y_train = shuffle(X_scale, y_train, random_state=0)
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_scale, y_train, test_size=0.2)
print(X_train.shape)
print(X_test.shape)
print(Y_train.shape)
print(Y_test.shape)

from sklearn.ensemble import GradientBoostingRegressor
regr = GradientBoostingRegressor(random_state=7)

param_grid = {
    'n_estimators': [220,250,300,350,400],
    'max_features': ['auto'],
    'max_depth' : [3,4,5,6,7,8],
    'criterion' :['friedman_mse','mse'],
    'learning_rate':[0.18,0.17,0.175,0.16]
}

from sklearn.model_selection import GridSearchCV
CV_rfc = GridSearchCV(estimator=regr, param_grid=param_grid, cv= 5)
CV_rfc.fit(X_train, Y_train)

CV_rfc.best_params_

rfc1=GradientBoostingRegressor(random_state=7, max_features='auto', n_estimators= 250, max_depth=5,criterion='friedman_mse',learning_rate=0.18)

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
scores = cross_val_score(rfc1, X_scale, y_train, cv=5)
print(scores)

print(scores.mean())

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
scores = cross_val_score(rfc1, X_scale, y_train, cv=5,scoring='neg_root_mean_squared_error')
print(scores)

print(scores.mean())

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
scores = cross_val_score(rfc1, X_scale, y_train, cv=5,scoring='neg_mean_absolute_error')
print(scores)

print(scores.mean())

rfc1.fit(X_train, Y_train)

rfc1.score(X_test,Y_test)

pred=rfc1.predict(X_test)

rfc1.score(X_train,Y_train)

from sklearn.metrics import mean_squared_error
print('RMSE:')
print(np.sqrt(mean_squared_error(Y_test,pred)))

from sklearn.metrics import mean_absolute_error
print(mean_absolute_error(Y_test,pred))

import matplotlib.pyplot as plt
plt.plot(Y_test,pred,"co")